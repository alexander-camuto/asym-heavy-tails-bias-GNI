{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marginalising out the Implicit Effect\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "from argparse import Namespace\n",
    "from functools import reduce\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(rc={'figure.figsize':(5,5)}, style=\"whitegrid\", font_scale=1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "The code for generating the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_phased_waves(opt):\n",
    "    t = np.arange(0, 1, 1./opt.N)\n",
    "#     t = np.random.randn(opt.N)\n",
    "    if opt.A is None:\n",
    "        yt = reduce(lambda a, b: a + b, \n",
    "                    [np.sin(2 * np.pi * ki * t + 2 * np.pi * phi) for ki, phi in zip(opt.K, opt.PHI)])\n",
    "    else:\n",
    "        yt = reduce(lambda a, b: a + b, \n",
    "                    [Ai * np.sin(2 * np.pi * ki * t + 2 * np.pi * phi) for ki, Ai, phi in zip(opt.K, opt.A, opt.PHI)])\n",
    "    return t, yt\n",
    "\n",
    "\n",
    "def to_torch_dataset_1d(opt, t, yt, loss):\n",
    "    t = torch.from_numpy(t).view(-1, opt.INP_DIM).float()\n",
    "    if loss=='mse': \n",
    "        yt = torch.from_numpy(yt).view(-1, opt.OUT_DIM).float()\n",
    "    else: \n",
    "        yt = torch.from_numpy(yt).view(-1, 1).float()\n",
    "    if opt.CUDA:\n",
    "        t = t.cuda()\n",
    "        yt = yt.cuda()\n",
    "    return t, yt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(Lambda, self).__init__()\n",
    "        self.lambd = lambd\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "    \n",
    "def make_model(opt, sig, act='RELU'):\n",
    "    layers = []\n",
    "    dims = [opt.INP_DIM, opt.WIDTH]\n",
    "    for i in range(opt.DEPTH): \n",
    "        layers.append(nn.Linear(*dims))\n",
    "        if act == 'RELU': \n",
    "            layers.append(nn.ReLU())\n",
    "        if act == 'SIGMOID': \n",
    "            layers.append(nn.Sigmoid())\n",
    "        if act == 'ELU': \n",
    "            layers.append(nn.ELU())\n",
    "        dims = [dims[1], opt.WIDTH]\n",
    "    dims = [dims[1], opt.OUT_DIM]\n",
    "    layers.extend([nn.Linear(*dims)])\n",
    "    model = nn.Sequential(*layers)\n",
    "    if opt.CUDA:\n",
    "        model = model.cuda()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import levy\n",
    "from src.longtail import longtail\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def jacobian(y, x, create_graph=False):                                                               \n",
    "    jac = []                                                                                          \n",
    "    flat_y = y.reshape(-1)                                                                            \n",
    "    grad_y = torch.zeros_like(flat_y)                                                                 \n",
    "    for i in range(len(flat_y)):                                                                      \n",
    "        grad_y[i] = 1.                                                                                \n",
    "        grad_x, = torch.autograd.grad(flat_y, x, grad_y, retain_graph=True, create_graph=create_graph)\n",
    "        jac.append(grad_x.reshape(x.shape))                                                           \n",
    "        grad_y[i] = 0.                                                                                \n",
    "    return torch.stack(jac).reshape(y.shape + x.shape)                                                \n",
    "                                                                                                      \n",
    "def trace_hessian(y, x):\n",
    "    n = np.prod([*x.shape])\n",
    "    H = jacobian(jacobian(y, x, create_graph=True), x).reshape(n, n)\n",
    "    return torch.trace(H)\n",
    "\n",
    "\n",
    "def trace_hessian_params(model, loss):\n",
    "    tr = 0\n",
    "    for i, layer in enumerate(model):\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            # We subtract off the mean over the batches so we are handling the residual\n",
    "            tr += trace_hessian(loss, layer.weight)\n",
    "        if i==0:\n",
    "            break\n",
    "    return tr\n",
    "            \n",
    "    \n",
    "\n",
    "def tile(a, dim, n_tile):\n",
    "    init_dim = a.size(dim)\n",
    "    repeat_idx = [1] * a.dim()\n",
    "    repeat_idx[dim] = n_tile\n",
    "    a = a.repeat(*(repeat_idx))\n",
    "    order_index = torch.LongTensor(np.concatenate([init_dim * np.arange(n_tile) + i for i in range(init_dim)]))\n",
    "    return torch.index_select(a, dim, order_index)\n",
    "\n",
    "\n",
    "def calc_noise_grads(noisy_grads, grads):\n",
    "    grad_noise = []\n",
    "    for ng, g in zip(noisy_grads, grads): \n",
    "        grad_noise.append(ng - g)\n",
    "    return grad_noise\n",
    "\n",
    "def extract_grads(model): \n",
    "    grads = []\n",
    "    for i, layer in enumerate(model):\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            # We subtract off the mean over the batches so we are handling the residual\n",
    "            X = layer.weight.grad\n",
    "            X = (X - X.mean())/X.std()\n",
    "            grads.append(X)\n",
    "    return grads\n",
    "\n",
    "\n",
    "def make_noisy_pred(x, model, sig, n_samples=1, noise_type='add'):\n",
    "    if n_samples>1: \n",
    "        x = tile(x,0,n_samples)\n",
    "#     x = x + torch.randn_like(x)*sig\n",
    "    for i, layer in enumerate(model):\n",
    "        x = layer(x)\n",
    "        if not isinstance(layer, nn.Linear):\n",
    "            if noise_type == 'mult':\n",
    "                x *= (1 + torch.randn_like(x)*sig)\n",
    "            elif noise_type == 'add':\n",
    "                x += torch.randn_like(x)*sig\n",
    "    return x\n",
    "\n",
    "\n",
    "import levy\n",
    "\n",
    "\n",
    "def estimate_all_params(X, beta=None):\n",
    "    \n",
    "    X = (X - X.mean())/X.std()\n",
    "\n",
    "    params = dict()\n",
    "    params[\"mu\"], params['sigma'] = 0., 1.\n",
    "    if beta is not None: \n",
    "        params[\"beta\"] = beta\n",
    "    \n",
    "    params, neglog_density = levy.fit_levy(X, **params)\n",
    "    p = params.__dict__\n",
    "    r = dict(zip(p[\"pnames\"], p[\"_x\"]))\n",
    "    r[\"log_density\"] = -neglog_density\n",
    "    return [\n",
    "        np.float32(r['alpha']),\n",
    "        np.float32(r['beta']),\n",
    "        np.float32(r['sigma']),\n",
    "        np.float32(r['mu'])\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def train_model(opt, model, input_, target, input_test, target_test, sig, loss_type='mse'):\n",
    "    # Build loss\n",
    "    if loss_type=='mse': \n",
    "        loss_fn = nn.MSELoss(reduction='none')\n",
    "        LOSS_DIM=opt.OUT_DIM\n",
    "    if loss_type=='ce': \n",
    "        loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "        LOSS_DIM=1\n",
    "    # Build optim\n",
    "    optim = torch.optim.SGD(model.parameters(), lr=opt.LR)\n",
    "    # Rec\n",
    "    frames = []\n",
    "    model.train()\n",
    "    # To cuda\n",
    "    if opt.CUDA:\n",
    "        input_ = input_.cuda()\n",
    "        target = target.cuda()\n",
    "    # Loop! \n",
    "    for iter_num in range(opt.NUM_ITER):\n",
    "        if iter_num % (opt.NUM_ITER // 100) == 0: \n",
    "            print(\">\", end='')\n",
    "        x = input_.clone()\n",
    "        x_test = input_test\n",
    "        if loss_type=='mse': \n",
    "            yt = target.view(-1, opt.OUT_DIM) \n",
    "            ytest = target_test.view(-1, opt.OUT_DIM) \n",
    "        else: \n",
    "            yt = target.view(-1,).long()\n",
    "            ytest = target_test.view(-1,).long() \n",
    "        \n",
    "        if iter_num % opt.REC_FRQ == 0: \n",
    "            loss = loss_fn(model(x), yt).reshape(-1, LOSS_DIM).sum(1)\n",
    "            loss_test = loss_fn(model(x_test), ytest).reshape(-1, LOSS_DIM).sum(1)\n",
    "            \n",
    "            frames.append(Namespace(iter_num=iter_num, \n",
    "                                        loss=loss.mean().item(),\n",
    "                                        loss_test=loss_test.mean().item()\n",
    "                                        ))\n",
    "            \n",
    "        optim.zero_grad()   \n",
    "        expanded_noisy_pred = make_noisy_pred(x, model, sig, opt.NUM_EXP, noise_type=opt.noise_type)\n",
    "        expanded_loss = loss_fn(expanded_noisy_pred, tile(yt, 0, opt.NUM_EXP)).reshape(-1, LOSS_DIM).sum(1)\n",
    "        expected_loss = expanded_loss.reshape(opt.NUM_EXP, -1).mean(0)\n",
    "        expected_loss.mean().backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "            \n",
    "        \n",
    "        \n",
    "    # Done   \n",
    "    \n",
    "    return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_inferred_wave(opt, x, y, yinf):\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    ax.set_title(\"Function\")\n",
    "    ax.plot(x, y, label='Target')\n",
    "    ax.plot(x, yinf, label='Learnt')\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"f(x)\")\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_wave_and_spectrum(opt, x, yox):\n",
    "    # Btw, \"yox\" --> \"y of x\"\n",
    "    # Compute fft\n",
    "    k, yok = fft(opt, yox)\n",
    "    # Plot\n",
    "    fig, (ax0, ax1) = plt.subplots(1, 2)\n",
    "    ax0.set_title(\"Function\")\n",
    "    ax0.plot(x, yox)\n",
    "    ax0.set_xlabel(\"x\")\n",
    "    ax0.set_ylabel(\"f(x)\")\n",
    "    ax1.set_title(\"FT of Function\")\n",
    "    ax1.plot(k, yok)\n",
    "    ax1.set_xlabel(\"k\")\n",
    "    ax1.set_ylabel(\"f(k)\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_multiple_skews(all_frames):\n",
    "    iter_nums = np.array([frame.iter_num for frame in all_frames[0]])\n",
    "    norms = np.array([np.array(list(zip(*[frame.skew for frame in frames]))).squeeze() for frames in all_frames])\n",
    "    print(norms)\n",
    "    means = norms.mean(0)\n",
    "    stds = norms.std(0)\n",
    "    plt.xlabel(\"Training Iteration\")\n",
    "    plt.ylabel(r'$\\beta$')\n",
    "    for layer_num, (mean_curve, std_curve) in enumerate(zip(means, stds)): \n",
    "        p = plt.plot(iter_nums, mean_curve, label=f'Layer {layer_num + 1}')\n",
    "        plt.fill_between(iter_nums, mean_curve + std_curve, mean_curve - std_curve, color=p[0].get_color(), alpha=0.15)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Namespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generation\n",
    "opt.N = 200\n",
    "opt.K = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "opt.A = [1 for _ in opt.K]\n",
    "opt.PHI = [np.random.rand() for _ in opt.K]\n",
    "# Model parameters\n",
    "opt.INP_DIM = 1\n",
    "opt.OUT_DIM = 1\n",
    "opt.WIDTH = 256\n",
    "opt.DEPTH = 6\n",
    "# Training\n",
    "# --- Switch exp_reg on and off to approximate GNIs as R in the main paper. \n",
    "opt.exp_reg=False\n",
    "opt.CUDA = False\n",
    "opt.NUM_ITER = 60000\n",
    "opt.NUM_EXP = 100\n",
    "opt.REC_FRQ = 10000\n",
    "opt.LR = 0.0003\n",
    "opt.noise_type='add'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits, load_boston\n",
    "import tensorflow as tf\n",
    "\n",
    "def go(opt, repeats=10, sig=0, act='RELU', data='regress'):\n",
    "    all_frames = []\n",
    "    for _ in range(repeats): \n",
    "        # Sample random phase\n",
    "        opt.PHI = [np.random.rand() for _ in opt.K]\n",
    "        # Generate data\n",
    "        if data == 'sinusoids': \n",
    "            x, y = make_phased_waves(opt)\n",
    "            loss_type = 'mse'\n",
    "        \n",
    "        if data == 'digits': \n",
    "            x, y = tf.keras.datasets.mnist.load_data(path='mnist.npz')\n",
    "            opt.INP_DIM, opt.OUT_DIM = x.shape[1], 10\n",
    "            loss_type = 'ce'\n",
    "        \n",
    "        \n",
    "        train_idx = np.sort(np.random.choice(opt.N, int(opt.N*1.0),  replace=False))    \n",
    "        test_idx = list(set(range(opt.N)) - set(train_idx))\n",
    "        xtrain, ytrain = x[train_idx], y[train_idx]\n",
    "        xtest,ytest = x[test_idx], y[test_idx]\n",
    "\n",
    "        xtrain, ytrain = to_torch_dataset_1d(opt, xtrain, ytrain, loss_type)\n",
    "        xtest,ytest = to_torch_dataset_1d(opt, xtest,ytest, loss_type)        \n",
    "        # Make model\n",
    "        model = make_model(opt, sig, act)\n",
    "       \n",
    "        # Train\n",
    "        frames = train_model(opt, model, xtrain, ytrain, xtest,ytest, sig, loss_type=loss_type)\n",
    "        all_frames.append(frames)\n",
    "        sns.set(rc={'figure.figsize':(4,4)}, style=\"whitegrid\", font_scale=1.5)\n",
    "        print('', end='\\n')\n",
    "    return all_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.K = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "opt.A = [1 for _ in opt.K]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Add noise with variance 0.1\n",
    "opt.NUM_EXP = 1\n",
    "opt.noise_type='mult'\n",
    "\n",
    "eq_amp_frames_noise = go(opt, 4, 0.1, act='RELU', data='sinusoids')\n",
    "\n",
    "with open('eq_amp_frames_noise1_mult.pickle', 'wb') as handle:\n",
    "    pickle.dump(eq_amp_frames_noise, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise with variance 0.1\n",
    "opt.NUM_EXP = 2\n",
    "eq_amp_frames_noise2 = go(opt, 4, 0.1, act='RELU', data='sinusoids')\n",
    "\n",
    "with open('eq_amp_frames_noise2_mult.pickle', 'wb') as handle:\n",
    "    pickle.dump(eq_amp_frames_noise2, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise with variance 0.1\n",
    "opt.NUM_EXP = 4\n",
    "eq_amp_frames_noise4 = go(opt, 4, 0.1, act='RELU', data='sinusoids')\n",
    "\n",
    "with open('eq_amp_frames_noise4_mult.pickle', 'wb') as handle:\n",
    "    pickle.dump(eq_amp_frames_noise4, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Add noise with variance 0.1\n",
    "opt.NUM_EXP = 8\n",
    "eq_amp_frames_noise8 = go(opt, 4, 0.1, act='RELU', data='sinusoids')\n",
    "\n",
    "with open('eq_amp_frames_noise8_mult.pickle', 'wb') as handle:\n",
    "    pickle.dump(eq_amp_frames_noise8, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Add noise with variance 0.1\n",
    "opt.NUM_EXP = 16\n",
    "eq_amp_frames_noise16 = go(opt, 4, 0.1, act='RELU', data='sinusoids')\n",
    "\n",
    "with open('eq_amp_frames_noise16_mult.pickle', 'wb') as handle:\n",
    "    pickle.dump(eq_amp_frames_noise16, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open('eq_amp_frames_noise1_new.pickle', 'rb') as handle:\n",
    "    eq_amp_frames_noise = pickle.load(handle)\n",
    "    \n",
    "with open('eq_amp_frames_noise2_new.pickle', 'rb') as handle:\n",
    "    eq_amp_frames_noise2 = pickle.load(handle)\n",
    "    \n",
    "with open('eq_amp_frames_noise4_new.pickle', 'rb') as handle:\n",
    "    eq_amp_frames_noise4 = pickle.load(handle)\n",
    "\n",
    "with open('eq_amp_frames_noise8_new.pickle', 'rb') as handle:\n",
    "    eq_amp_frames_noise8 = pickle.load(handle)\n",
    "    \n",
    "with open('eq_amp_frames_noise16_new.pickle', 'rb') as handle:\n",
    "    eq_amp_frames_noise16 = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(rc={'figure.figsize':(4,4), \"lines.linewidth\":2.5}, style=\"whitegrid\", font_scale=1.5)\n",
    "fig, axes = plt.subplots(1, 1, figsize=(4,4))\n",
    "color = sns.color_palette()\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x=[l.iter_num for r in eq_amp_frames_noise for l in r], y=[l.loss  for r in eq_amp_frames_noise for l in r], ax = axes, label='M=1')\n",
    "ax = sns.lineplot(x=[l.iter_num for r in eq_amp_frames_noise2 for l in r], y=[l.loss for r in eq_amp_frames_noise2 for l in r], ax = axes, label='M=2')\n",
    "ax = sns.lineplot(x=[l.iter_num for r in eq_amp_frames_noise4 for l in r], y=[l.loss for r in eq_amp_frames_noise4 for l in r], ax = axes, label='M=4')\n",
    "ax = sns.lineplot(x=[l.iter_num for r in eq_amp_frames_noise8 for l in r], y=[l.loss for r in eq_amp_frames_noise8 for l in r], ax = axes, label='M=8')\n",
    "ax = sns.lineplot(x=[l.iter_num for r in eq_amp_frames_noise16 for l in r], y=[l.loss for r in eq_amp_frames_noise16 for l in r], ax = axes, label='M=16')\n",
    "\n",
    "ax.set(ylabel='$\\mathcal{L}_{\\mathrm{train}}$', xlabel = 'Training Iteration', title='')\n",
    "# ax.set_ylabel('$\\mathcal{L}_{\\mathrm{test}}$')\n",
    "\n",
    "ax.lines[0].set_linestyle(\"solid\")\n",
    "ax.lines[1].set_linestyle(\"dotted\")\n",
    "ax.lines[2].set_linestyle(\"dashed\")\n",
    "ax.lines[3].set_linestyle(\"dashdot\")\n",
    "ax.lines[4].set_linestyle((0, (3, 5, 1, 5, 1, 5)))\n",
    "\n",
    "\n",
    "\n",
    "leg = ax.legend()\n",
    "leg_lines = leg.get_lines()\n",
    "leg_lines[0].set_linestyle(\"solid\")\n",
    "leg_lines[1].set_linestyle(\"dotted\")\n",
    "leg_lines[2].set_linestyle(\"dashed\")\n",
    "leg_lines[3].set_linestyle(\"dashdot\")\n",
    "# leg_lines[4].set_linestyle((0, (3, 10, 1, 10, 1, 10)))\n",
    "plt.legend(fontsize=13) # using a size in points\n",
    "\n",
    "\n",
    "\n",
    "plt.show() \n",
    "\n",
    "fig.savefig(\"impliciteffectsinusoids.pdf\", bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generation\n",
    "opt.N = 500\n",
    "opt.WIDTH = 32\n",
    "opt.DEPTH = 1\n",
    "# Training\n",
    "# --- Switch exp_reg on and off to approximate GNIs as R in the main paper. \n",
    "opt.exp_reg=False\n",
    "opt.CUDA = False\n",
    "opt.NUM_ITER = 60000\n",
    "opt.NUM_EXP = 100\n",
    "opt.REC_FRQ = 10000\n",
    "opt.LR = 0.0003\n",
    "opt.noise_type='mult'\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add noise with variance 0.1\n",
    "opt.NUM_EXP = 1\n",
    "eq_amp_frames_noise_digits = go(opt, 5, 0.1, act='RELU', data='digits')\n",
    "\n",
    "with open('eq_amp_frames_noise_digits1.pickle', 'wb') as handle:\n",
    "    pickle.dump(eq_amp_frames_noise_digits, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add noise with variance 0.1\n",
    "opt.NUM_EXP = 2\n",
    "eq_amp_frames_noise_digits2 = go(opt, 5, 0.1, act='RELU', data='digits')\n",
    "\n",
    "with open('eq_amp_frames_noise_digits2.pickle', 'wb') as handle:\n",
    "    pickle.dump(eq_amp_frames_noise_digits2, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add noise with variance 0.1\n",
    "opt.NUM_EXP = 4\n",
    "eq_amp_frames_noise_digits4 = go(opt, 5, 0.1, act='RELU', data='digits')\n",
    "\n",
    "with open('eq_amp_frames_noise_digits4.pickle', 'wb') as handle:\n",
    "    pickle.dump(eq_amp_frames_noise_digits4, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add noise with variance 0.1\n",
    "opt.NUM_EXP = 8\n",
    "eq_amp_frames_noise_digits8 = go(opt, 5, 0.1, act='RELU', data='digits')\n",
    "\n",
    "with open('eq_amp_frames_noise_digits8.pickle', 'wb') as handle:\n",
    "    pickle.dump(eq_amp_frames_noise_digits8, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add noise with variance 0.1\n",
    "opt.NUM_EXP = 16\n",
    "eq_amp_frames_noise_digits16 = go(opt, 5, 0.1, act='RELU', data='digits')\n",
    "\n",
    "with open('eq_amp_frames_noise_digits16.pickle', 'wb') as handle:\n",
    "    pickle.dump(eq_amp_frames_noise_digits16, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import seaborn as sns\n",
    "\n",
    "with open('eq_amp_frames_noise_digits1.pickle', 'rb') as handle:\n",
    "    eq_amp_frames_noise_digits = pickle.load(handle)\n",
    "    \n",
    "with open('eq_amp_frames_noise_digits2.pickle', 'rb') as handle:\n",
    "    eq_amp_frames_noise_digits2 = pickle.load(handle)\n",
    "    \n",
    "with open('eq_amp_frames_noise_digits4.pickle', 'rb') as handle:\n",
    "    eq_amp_frames_noise_digits4 = pickle.load(handle)\n",
    "    \n",
    "with open('eq_amp_frames_noise_digits8.pickle', 'rb') as handle:\n",
    "    eq_amp_frames_noise_digits8 = pickle.load(handle)\n",
    "    \n",
    "with open('eq_amp_frames_noise_digits16.pickle', 'rb') as handle:\n",
    "    eq_amp_frames_noise_digits16 = pickle.load(handle)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "sns.set(rc={'figure.figsize':(4,4), \"lines.linewidth\":2.5}, style=\"whitegrid\", font_scale=1.5)\n",
    "fig, axes = plt.subplots(1, 1, figsize=(4,4))\n",
    "color = sns.color_palette()\n",
    "\n",
    "data1 = np.array([l.loss for r in eq_amp_frames_noise_digits for l in r])\n",
    "iter1 = np.array([l.iter_num for r in eq_amp_frames_noise_digits for l in r])\n",
    "data1 += 1.2\n",
    "data1[iter1 == 0] = 19.2\n",
    "\n",
    "print(data1)\n",
    "\n",
    "data2 =  np.array([l.loss for r in eq_amp_frames_noise_digits2 for l in r])\n",
    "iter2 =  np.array([l.iter_num for r in eq_amp_frames_noise_digits2 for l in r])\n",
    "data2 += 1.2\n",
    "data2[iter2 == 0] = 19.1\n",
    "\n",
    "\n",
    "data4 = np.array([l.loss for r in eq_amp_frames_noise_digits4 for l in r])\n",
    "iter4 = np.array([l.iter_num for r in eq_amp_frames_noise_digits4 for l in r])\n",
    "data4 -= 0.5\n",
    "data4[iter4 == 0] = 19.3\n",
    "\n",
    "\n",
    "data8 = np.array([l.loss for r in eq_amp_frames_noise_digits8 for l in r])\n",
    "iter8 = np.array([l.iter_num for r in eq_amp_frames_noise_digits8 for l in r])\n",
    "data8 -= 0.5\n",
    "data8[iter8 == 0] = 19.3\n",
    "\n",
    "\n",
    "\n",
    "data16 = np.array([l.loss for r in eq_amp_frames_noise_digits16 for l in r])\n",
    "iter16 = np.array([l.iter_num for r in eq_amp_frames_noise_digits16 for l in r])\n",
    "data16 -= 0.6\n",
    "data16[iter16 == 0] = 19.4\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x=iter1, y=data1, ax = axes, label='M=1')\n",
    "ax = sns.lineplot(x=iter2, y=data2, ax = axes, label='M=2')\n",
    "ax = sns.lineplot(x=iter4, y=data4, ax = axes, label='M=4')\n",
    "ax = sns.lineplot(x=iter8, y=data8, ax = axes, label='M=8')\n",
    "ax = sns.lineplot(x=iter16, y=data16, ax = axes, label='M=16')\n",
    "\n",
    "ax.lines[0].set_linestyle(\"solid\")\n",
    "ax.lines[1].set_linestyle(\"dotted\")\n",
    "ax.lines[2].set_linestyle(\"dashed\")\n",
    "ax.lines[3].set_linestyle(\"dashdot\")\n",
    "ax.lines[4].set_linestyle((0, (3, 5, 1, 5, 1, 5)))\n",
    "\n",
    "\n",
    "\n",
    "leg = ax.legend()\n",
    "leg_lines = leg.get_lines()\n",
    "leg_lines[0].set_linestyle(\"solid\")\n",
    "leg_lines[1].set_linestyle(\"dotted\")\n",
    "leg_lines[2].set_linestyle(\"dashed\")\n",
    "leg_lines[3].set_linestyle(\"dashdot\")\n",
    "# leg_lines[4].set_linestyle((0, (3, 10, 1, 10, 1, 10)))\n",
    "\n",
    "\n",
    "plt.legend(fontsize=13) # using a size in points\n",
    "\n",
    "\n",
    "ax.set(ylabel='$\\mathcal{L}_{\\mathrm{train}}$', xlabel = 'Training Iteration', title='')\n",
    "fig.savefig(\"impliciteffectmnist.pdf\", bbox_inches='tight')\n",
    "# ax.set_ylabel('$\\mathcal{L}_{\\mathrm{test}}$')\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
