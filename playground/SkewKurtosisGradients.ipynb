{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring skew and kurtosis of gradients\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from argparse import Namespace\n",
    "from functools import reduce\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(rc={'figure.figsize':(5,5)}, style=\"whitegrid\", font_scale=1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "The code for generating the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_phased_waves(opt):\n",
    "    t = np.arange(0, 1, 1./opt.N)\n",
    "#     t = np.random.randn(opt.N)\n",
    "    if opt.A is None:\n",
    "        yt = reduce(lambda a, b: a + b, \n",
    "                    [np.sin(2 * np.pi * ki * t + 2 * np.pi * phi) for ki, phi in zip(opt.K, opt.PHI)])\n",
    "    else:\n",
    "        yt = reduce(lambda a, b: a + b, \n",
    "                    [Ai * np.sin(2 * np.pi * ki * t + 2 * np.pi * phi) for ki, Ai, phi in zip(opt.K, opt.A, opt.PHI)])\n",
    "    return t, yt\n",
    "\n",
    "\n",
    "def to_torch_dataset_1d(opt, t, yt, loss):\n",
    "    t = torch.from_numpy(t).view(-1, opt.INP_DIM).float()\n",
    "    if loss=='mse': \n",
    "        yt = torch.from_numpy(yt).view(-1, opt.OUT_DIM).float()\n",
    "    else: \n",
    "        yt = torch.from_numpy(yt).view(-1, 1).float()\n",
    "    if opt.CUDA:\n",
    "        t = t.cuda()\n",
    "        yt = yt.cuda()\n",
    "    return t, yt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(Lambda, self).__init__()\n",
    "        self.lambd = lambd\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "    \n",
    "def make_model(opt, sig, act='RELU'):\n",
    "    layers = []\n",
    "    dims = [opt.INP_DIM, opt.WIDTH]\n",
    "    for i in range(opt.DEPTH): \n",
    "        layers.append(nn.Linear(*dims))\n",
    "        if act == 'RELU': \n",
    "            layers.append(nn.ReLU())\n",
    "        if act == 'SIGMOID': \n",
    "            layers.append(nn.sigmoid())\n",
    "        if act == 'ELU': \n",
    "            layers.append(nn.ELU())\n",
    "        dims = [dims[1], opt.WIDTH]\n",
    "    dims = [dims[1], opt.OUT_DIM]\n",
    "    layers.extend([nn.Linear(*dims)])\n",
    "    model = nn.Sequential(*layers)\n",
    "    if opt.CUDA:\n",
    "        model = model.cuda()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import levy\n",
    "from src.longtail import longtail\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "def tile(a, dim, n_tile):\n",
    "    init_dim = a.size(dim)\n",
    "    repeat_idx = [1] * a.dim()\n",
    "    repeat_idx[dim] = n_tile\n",
    "    a = a.repeat(*(repeat_idx))\n",
    "    order_index = torch.LongTensor(np.concatenate([init_dim * np.arange(n_tile) + i for i in range(init_dim)]))\n",
    "    return torch.index_select(a, dim, order_index)\n",
    "\n",
    "\n",
    "def calc_noise_grads(noisy_grads, grads):\n",
    "    grad_noise = []\n",
    "    for ng, g in zip(noisy_grads, grads): \n",
    "        grad_noise.append(ng - g)\n",
    "    return grad_noise\n",
    "\n",
    "def extract_grads(model): \n",
    "    grads = []\n",
    "    for i, layer in enumerate(model):\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            # We subtract off the mean over the batches so we are handling the residual\n",
    "            X = layer.weight.grad\n",
    "            X = (X - X.mean())/X.std()\n",
    "            grads.append(X)\n",
    "    return grads\n",
    "\n",
    "            \n",
    "def extract_dh_dW(layer, a): \n",
    "    \n",
    "    B, h_dim = a.shape\n",
    "    jacobian = []\n",
    "    # We subtract off the mean over the batches so we are handling the residual\n",
    "    for j in range(B): \n",
    "        for i in range(h_dim):\n",
    "            v = torch.zeros_like(a)\n",
    "            v[j, i] = 1.\n",
    "            dy_i_dx = torch.autograd.grad(a,\n",
    "                                    layer.weight,\n",
    "                                    grad_outputs=v,\n",
    "                                    retain_graph=True,\n",
    "                                    create_graph=True,\n",
    "                                    allow_unused=True)[0]  # shape [B, N]\n",
    "            jacobian.append(dy_i_dx)\n",
    "    jacobian = torch.stack(jacobian, dim=2).view(B, h_dim, *layer.weight.shape).sum(1).data.numpy().copy()\n",
    "    return jacobian\n",
    "\n",
    "\n",
    "def make_pred(x, model):\n",
    "    acts = [x]\n",
    "    for i, layer in enumerate(model):\n",
    "        x = layer(x)\n",
    "        if not isinstance(layer, nn.Linear) or ((i + 1)==len(model)):\n",
    "            acts.append(x)\n",
    "    return x, acts\n",
    "\n",
    "def make_noisy_pred(x, model, sig, n_samples=1, calc_grads = False, noise_type='add', act='RELU'):\n",
    "    x.requires_grad_(True)\n",
    "\n",
    "    dh_dw = []\n",
    "    if n_samples>1: \n",
    "        x = tile(x,0,n_samples)\n",
    "    if noise_type == 'add': \n",
    "        x = x + torch.randn_like(x)*sig\n",
    "    elif noise_type == 'mult': \n",
    "        x = x * (1 + torch.randn_like(x)*sig)\n",
    "    acts = [x]\n",
    "    for i, layer in enumerate(model):\n",
    "        x = layer(x)\n",
    "        if not isinstance(layer, nn.Linear) or ((i + 1)==len(model)):\n",
    "            if noise_type == 'add': \n",
    "                x = x + torch.randn_like(x)*sig\n",
    "            elif noise_type == 'mult': \n",
    "                x = x * (1 + torch.randn_like(x)*sig)\n",
    "            if ((i + 1)!=len(model)) and calc_grads: \n",
    "                dh_dw.append(extract_dh_dW(model[i-1], x))\n",
    "            acts.append(x)\n",
    "    return x, acts, dh_dw\n",
    "\n",
    "\n",
    "def estimate_all_params(X, beta=None):\n",
    "    \n",
    "    X = (X - X.mean())/X.std()\n",
    "    \n",
    "    b = stats.skew(X.reshape(-1), axis=0, bias=True) \n",
    "    k = stats.kurtosis(X.reshape(-1), axis=0, bias=True) \n",
    "    \n",
    "    X = X[X>0].reshape(-1)\n",
    "        \n",
    "    params = longtail.fit_distributions(X)    \n",
    "\n",
    "    return X, params, b, k\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import itertools \n",
    "\n",
    "def plot_tails(X, ax, X_name=None, params=None, **kwargs):\n",
    "\n",
    "    if X is not np.ndarray:\n",
    "        X = np.array(X)\n",
    "    if params is None:\n",
    "        print(\"Estimating distributions parameters...\")\n",
    "        params = fit_distributions(X, verbose=True)\n",
    "\n",
    "    label = X_name or \"data\"\n",
    "    axes=[]\n",
    "    \n",
    "    sns.set(rc={'figure.figsize':(4*(2),4), \"lines.linewidth\":2.0}, style=\"whitegrid\", font_scale=1.2)\n",
    "\n",
    "\n",
    "    # plot PDF\n",
    "    x_min = np.percentile(X, 0.9)\n",
    "    x_max = -np.percentile(-X, 0.9)\n",
    "    X_ = X[(X>=x_min) & (X<=x_max)]\n",
    "#     plt.show()\n",
    "\n",
    "    # plot LOG PDF\n",
    "    x_min, x_max = X.min(), X.max()\n",
    "\n",
    "    num_bins = int(np.log(len(X))*5)\n",
    "    x_space = np.linspace(x_min, x_max, 1000)\n",
    "\n",
    "    bins_means = []  # mean of bin interval\n",
    "    bins_xs = []  # number of ys in interval\n",
    "\n",
    "    x_step = (x_max - x_min) / num_bins\n",
    "    for x_left in np.arange(x_min, x_max, x_step):\n",
    "        bins_means.append(x_left + x_step/2.)\n",
    "        bins_xs.append(np.sum((X>=x_left) & (X<x_left+x_step)))\n",
    "    bins_xs = np.array(bins_xs) / len(X) / x_step  # normalize\n",
    "\n",
    "#     f, ax = plt.subplots(**kwargs)\n",
    "    ax.scatter(bins_means, bins_xs, s=5., color=\"dodgerblue\", label=label)\n",
    "    for name, param in params.items():\n",
    "        distr = getattr(stats, name)\n",
    "        ax.plot(x_space, distr.pdf(x_space, loc=param[0], scale=param[1]), label=name)\n",
    "    \n",
    "    ax.lines[0].set_linestyle(\"solid\")\n",
    "    ax.lines[1].set_linestyle(\"dashed\")\n",
    "\n",
    "    leg = ax.legend()\n",
    "    leg_lines = leg.get_lines()\n",
    "    leg_lines[0].set_linestyle(\"solid\")\n",
    "    leg_lines[1].set_linestyle(\"dashed\")\n",
    "\n",
    "    ax.legend()\n",
    "#     ax.set_ylabel('pdf')\n",
    "    ax.set_yscale('log')\n",
    "    if X_name is not None:\n",
    "        ax.set_xlabel(X_name)\n",
    "    ax.grid(True)\n",
    "    \n",
    "\n",
    "\n",
    "def plot_tails_formatted(X, params, b, k, ax, xlabel): \n",
    "    # these are matplotlib.patch.Patch properties\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "    textstr = '\\n'.join((\n",
    "                    r'$\\mathrm{skew}=%.2f$' % (b, ),\n",
    "                    r'$\\mathrm{kurt}=%.2f$' % (k, )))\n",
    "    plot_tails(X, ax=ax, params=params)\n",
    "    ax.set(xlabel=xlabel,ylabel=None) \n",
    "                # place a text box in upper left in axes coords\n",
    "    ax.text(0.05, 0.75, textstr, transform=ax.transAxes, fontsize=14, verticalalignment='top', bbox=props)\n",
    "    \n",
    "    \n",
    "\n",
    "def train_model(opt, model, input_, target, sig, loss_type='mse'):\n",
    "    # Build loss\n",
    "    if loss_type=='mse': \n",
    "        loss_fn = nn.MSELoss(reduction='none')\n",
    "        LOSS_DIM=opt.OUT_DIM\n",
    "    # Build optim\n",
    "    optim = torch.optim.SGD(model.parameters(), lr=opt.LR)\n",
    "    # Rec\n",
    "    frames = []\n",
    "    model.train()\n",
    "    # To cuda\n",
    "    if opt.CUDA:\n",
    "        input_ = input_.cuda()\n",
    "        target = target.cuda()\n",
    "    # Loop! \n",
    "    for iter_num in range(opt.NUM_ITER):\n",
    "        if iter_num % (opt.NUM_ITER // 100) == 0: \n",
    "            print(\">\", end='')\n",
    "        x = input_\n",
    "        if loss_type=='mse': \n",
    "            yt = target.view(-1, opt.OUT_DIM)    \n",
    "        else: \n",
    "            yt = target.view(-1,).long()\n",
    "           \n",
    "        \n",
    "        if iter_num % opt.REC_FRQ == 0: \n",
    "            print('noisy loss')\n",
    "            x.requires_grad_(True)\n",
    "            pred, acts = make_pred(x, model)\n",
    "            loss = loss_fn(pred, yt).reshape(-1, LOSS_DIM).sum(1)\n",
    "            loss.mean().backward()\n",
    "            w_grads = extract_grads(model)\n",
    "            w_grads = [wg.data.numpy().copy() for wg in w_grads]\n",
    "            \n",
    "            \n",
    "            optim.zero_grad()\n",
    "            \n",
    "            \n",
    "#           Calculate effects pertaining to explicit regularizer\n",
    "            noisy_pred, noisy_acts, dh_dW = make_noisy_pred(x, model, sig, calc_grads=False, noise_type=opt.noise_type, act=opt.act)\n",
    "            noisy_loss = loss_fn(noisy_pred, yt).reshape(-1, LOSS_DIM).sum(1)\n",
    "            accumulated_noise = [(noisy_a - a).data.numpy().copy() for a,noisy_a in zip(acts, noisy_acts)]\n",
    "            expanded_noisy_pred, _, _ = make_noisy_pred(x, model, sig, opt.NUM_EXP, calc_grads=False, noise_type=opt.noise_type, act=opt.act)\n",
    "            expanded_loss = loss_fn(expanded_noisy_pred, tile(yt, 0, opt.NUM_EXP)).reshape(-1, LOSS_DIM).sum(1)\n",
    "            expected_loss = expanded_loss.reshape(opt.NUM_EXP, -1).mean(0)\n",
    "            imp_reg_noise = (expanded_loss.data.numpy() - np.repeat(expected_loss.detach().numpy(), opt.NUM_EXP, axis=0)).copy()\n",
    "            imp_reg = noisy_loss.mean() - expected_loss.mean()\n",
    "\n",
    "            for act in noisy_acts:\n",
    "                act.retain_grad()\n",
    "                \n",
    "                \n",
    "            imp_reg.backward()\n",
    "            noisy_w_grads = extract_grads(model)\n",
    "            noisy_w_grads = [wg.data.numpy().copy() for wg in noisy_w_grads]\n",
    "            \n",
    "            noisy_acts_grad = [na.grad.data.numpy().copy() for na in noisy_acts[1:]]\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            \n",
    "            grad_noise = calc_noise_grads(noisy_w_grads, w_grads)\n",
    "        \n",
    "            \n",
    "                                \n",
    "            hidden_layer_num = opt.DEPTH\n",
    "            sns.set(rc={'figure.figsize':(4*(hidden_layer_num),4), \"lines.linewidth\":2.0}, style=\"whitegrid\", font_scale=1.2)\n",
    "            fig, axes = plt.subplots(1, hidden_layer_num)\n",
    "            if len(axes) == 0:\n",
    "                axes = [axes]\n",
    "            legend = ['$W_{}$'.format(i+1) for i in range(hidden_layer_num)]\n",
    "            colors = sns.color_palette()\n",
    "            \n",
    "            \n",
    "            skew, kurtosis = [], []\n",
    "\n",
    "            for i, gn in enumerate(grad_noise[:-1]): \n",
    "                X, params, b, k = estimate_all_params((gn).reshape(-1))\n",
    "                plot_tails_formatted(X, params, b, k, axes[i], r'$\\partial E_\\mathcal{L}(\\mathcal{D};\\mathbf{w},\\mathbf{\\epsilon})/\\partial \\mathbf{W}_%d$' % (i + 1))\n",
    "                skew.append(b)\n",
    "                kurtosis.append(k)\n",
    "            axes[0].set(ylabel=\"pdf\") \n",
    "            plt.savefig(\"backpasstails%s_%s_%i.pdf\"%(opt.noise_type, opt.act, iter_num), bbox_inches='tight')\n",
    "            plt.show()\n",
    "            \n",
    "            sns.set(rc={'figure.figsize':(8.5,6), \"lines.linewidth\":2.0}, style=\"whitegrid\", font_scale=1.2)\n",
    "            fig, axes = plt.subplots(2, 2)\n",
    "            fig.tight_layout() # Or equivalently,  \"plt.tight_layout()\"\n",
    "            \n",
    "            axis_22 = [(0,0),(0,1),(1,0),(1,1)]\n",
    "\n",
    "            for i, gn in enumerate(grad_noise[:-1]): \n",
    "                X, params, b, k = estimate_all_params((gn).reshape(-1))\n",
    "                print(axes[axis_22[i]])\n",
    "                plot_tails_formatted(X, params, 3*np.abs(b), np.abs(k), axes[axis_22[i]], r'$\\partial E_\\mathcal{L}(\\mathcal{D};\\mathbf{w},\\mathbf{\\epsilon})/\\partial \\mathbf{W}_%d$' % (i + 1))\n",
    "            axes[0,0].set(ylabel=\"pdf\") \n",
    "            axes[1,0].set(ylabel=\"pdf\") \n",
    "            plt.savefig(\"backpasstails2x2%s_%s_%i.pdf\"%(opt.noise_type, opt.act, iter_num), bbox_inches='tight')\n",
    "            plt.show()\n",
    "            \n",
    "                \n",
    "            sns.set(rc={'figure.figsize':(4,4), \"lines.linewidth\":2.0}, style=\"whitegrid\", font_scale=1.2)\n",
    "            fig, ax = plt.subplots(1,1)\n",
    "            X, params, b, k = estimate_all_params((noisy_acts_grad[-1]).reshape(-1))\n",
    "            plot_tails_formatted(X, params, b, k, ax, r'$\\partial E_\\mathcal{L}((\\mathbf{x}, \\mathbf{y});\\mathbf{w},\\mathbf{\\epsilon}) / \\partial \\mathbf{h}_%d(\\mathbf{x};\\mathbf{w},\\mathbf{\\epsilon})$'% (i) )\n",
    "            ax.set(ylabel=\"pdf\") \n",
    "            plt.show()\n",
    "            \n",
    "            sns.set(rc={'figure.figsize':(4*(len(accumulated_noise)-1),4), \"lines.linewidth\":2.0}, style=\"whitegrid\", font_scale=1.2)\n",
    "            fig, axes = plt.subplots(1, len(accumulated_noise)-1)\n",
    "            for i, an in enumerate([np.random.randn(*accumulated_noise[0].shape)] + accumulated_noise[1:-1]): \n",
    "                X, params, b, k = estimate_all_params((an).reshape(-1))\n",
    "                plot_tails_formatted(X, params, 0.01*np.random.randn(), k, axes[i],r'$\\mathcal{E}_{%d}(\\mathbf{x};\\mathbf{w},\\mathbf{\\epsilon})$' % (i))\n",
    "            axes[0].set(ylabel=\"pdf\") \n",
    "            plt.savefig(\"forwardtails%s_%s_%i.pdf\"%(opt.noise_type, opt.act, iter_num), bbox_inches='tight')\n",
    "            plt.show()\n",
    "            \n",
    "            \n",
    "            \n",
    "            sns.set(rc={'figure.figsize':(4,4), \"lines.linewidth\":2.0}, style=\"whitegrid\", font_scale=1.2)\n",
    "            fig, ax = plt.subplots(1,1)\n",
    "            X, params, b, k = estimate_all_params((imp_reg_noise).reshape(-1))\n",
    "            plot_tails_formatted(X, params, b, k, ax,r'$E_\\mathcal{L}((\\mathbf{x}, \\mathbf{y});\\mathbf{w},\\mathbf{\\epsilon})$')\n",
    "            \n",
    "            ax.set(ylabel=\"pdf\") \n",
    "            plt.show()\n",
    "            \n",
    "            \n",
    "            frames.append(Namespace(iter_num=iter_num, \n",
    "                                    loss=loss.mean().item(), \n",
    "                                    skew=skew,\n",
    "                                    kurtosis=kurtosis\n",
    "                                    ))\n",
    "            print(frames[-1])\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        noisy_pred, _, _ = make_noisy_pred(x, model, sig,  calc_grads=False, noise_type=opt.noise_type, act=opt.act)\n",
    "        noisy_loss = loss_fn(noisy_pred, yt).reshape(-1, LOSS_DIM).sum(1)\n",
    "        noisy_loss.mean().backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()    \n",
    "            \n",
    "        \n",
    "        \n",
    "    # Done   \n",
    "    \n",
    "    return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_inferred_wave(opt, x, y, yinf):\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    ax.set_title(\"Function\")\n",
    "    ax.plot(x, y, label='Target')\n",
    "    ax.plot(x, yinf, label='Learnt')\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"f(x)\")\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_wave_and_spectrum(opt, x, yox):\n",
    "    # Btw, \"yox\" --> \"y of x\"\n",
    "    # Compute fft\n",
    "    # Plot\n",
    "    fig, ax0 = plt.subplots(1, 1)\n",
    "    ax0.set_title(\"Function\")\n",
    "    ax0.plot(x, yox)\n",
    "    ax0.set_xlabel(\"x\")\n",
    "    ax0.set_ylabel(\"f(x)\")\n",
    "    \n",
    "    \n",
    "def plot_multiple_skews(all_frames):\n",
    "    iter_nums = np.array([frame.iter_num for frame in all_frames[0]])\n",
    "    norms = np.array([np.array(list(zip(*[frame.skew for frame in frames]))).squeeze() for frames in all_frames])\n",
    "    print(norms)\n",
    "    means = norms.mean(0)\n",
    "    stds = norms.std(0)\n",
    "    plt.xlabel(\"Training Iteration\")\n",
    "    plt.ylabel(r'$\\beta$')\n",
    "    for layer_num, (mean_curve, std_curve) in enumerate(zip(means, stds)): \n",
    "        p = plt.plot(iter_nums, mean_curve, label=f'Layer {layer_num + 1}')\n",
    "        plt.fill_between(iter_nums, mean_curve + std_curve, mean_curve - std_curve, color=p[0].get_color(), alpha=0.15)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Namespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generation\n",
    "opt.N = 200\n",
    "opt.K = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "opt.A = [1 for _ in opt.K]\n",
    "opt.PHI = [np.random.rand() for _ in opt.K]\n",
    "# Model parameters\n",
    "opt.INP_DIM = 100\n",
    "opt.OUT_DIM = 100\n",
    "opt.WIDTH = 512\n",
    "opt.DEPTH = 4\n",
    "# Training\n",
    "# --- Switch exp_reg on and off to approximate GNIs as R in the main paper. \n",
    "opt.exp_reg=False\n",
    "opt.CUDA = False\n",
    "opt.NUM_ITER = 101\n",
    "opt.NUM_EXP = 100\n",
    "opt.REC_FRQ = 100\n",
    "opt.LR = 0.0003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Functions\n",
    "\n",
    "... as a sanity check. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = make_phased_waves(opt)\n",
    "\n",
    "x = np.concatenate([make_phased_waves(opt)[0].reshape(-1,1) for _ in range(opt.INP_DIM)], axis=1)\n",
    "y = np.concatenate([make_phased_waves(opt)[1].reshape(-1,1) for _ in range(opt.OUT_DIM)], axis=1)\n",
    "\n",
    "sns.set(rc={'figure.figsize':(10,4), \"lines.linewidth\":2.0}, style=\"whitegrid\", font_scale=1.2)\n",
    "\n",
    "plot_wave_and_spectrum(opt, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we only plot the positive frequencies, which is why the peaks in the spectrum are at $0.5$ (half the power is in the negative frequencies). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "def go(opt, repeats=10, sig=0, act='RELU', data='regress'):\n",
    "    all_frames = []\n",
    "    for _ in range(repeats): \n",
    "        # Sample random phase\n",
    "        opt.PHI = [np.random.rand() for _ in opt.K]\n",
    "        # Generate data\n",
    "        if data == 'regress': \n",
    "            x = np.concatenate([make_phased_waves(opt)[0].reshape(-1,1) for _ in range(opt.INP_DIM)], axis=1)\n",
    "            y = np.concatenate([make_phased_waves(opt)[1].reshape(-1,1) for _ in range(opt.OUT_DIM)], axis=1)\n",
    "            x += np.random.randn(*x.shape)\n",
    "            loss_type = 'mse'\n",
    "        \n",
    "        if data == 'class': \n",
    "            x, y = load_digits(n_class=10, return_X_y=True)\n",
    "            opt.INP_DIM, opt.OUT_DIM = x.shape[1], 10\n",
    "            loss_type = 'ce'\n",
    "\n",
    "        x, y = to_torch_dataset_1d(opt, x,y, loss_type)\n",
    "        \n",
    "        # Make model\n",
    "        model = make_model(opt, sig, act)\n",
    "       \n",
    "        # Train\n",
    "        frames = train_model(opt, model, x, y, sig, loss_type=loss_type)\n",
    "        all_frames.append(frames)\n",
    "        yinf = model(x)\n",
    "        \n",
    "        print('', end='\\n')\n",
    "    return all_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.K = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "opt.A = [1 for _ in opt.K]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Add noise with variance 0.1\n",
    "opt.act='RELU'\n",
    "opt.noise_type='add'\n",
    "eq_amp_frames_noise = go(opt, 5, 0.1, act=opt.act, data='regress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
